## Cluster Maintenance:
- Cluster Upgrade Process
- Operating system upgrades
- Backup and restore methodologies


### 1. OS Upgrades:
- Whenever the node went down, the pods that are running will go offline .
- Those pods that are part of replicaset will recreate this pods in another nodes and those which are not part of replicaset will not be scheduled . and after pod eviction times , only empty node will come up without any pods in that paticular node.
- `Pod eviction Timing`: The time in which master nodes wait for the pod to come back to online state after it went offline, ~5 min default value
- So, if we are not sure about the node that come back online within 5 mins, we cannot do upgardes directly on the nodes.
- Safer way is, `drain` the nodes
    - Doing this all the pods will be moved to another nodes safely.
    - also node is marked as `unSchedule` meaning no pods is scheduled on this node.
    ```
    kubectl drain <node-name>
    kubectl drain <node-name> --ignore-daemonsets  // in case dqemonsets are available
    ```
    - `Cordon`: it simply mark nodes as unschdule , unless like drain it moves pods to other nodes
    ```
    kubectl cordon <node_name>
    ```
    - `uncordon`: to set back the node to normal schduling
    ```
    kubectl uncordon <node_name>
    ```
Notes:
-  When control plaine does not have any taints , still when node is cordoned , pods will be scheduled in controlplane.
- when nodes ocntains a pods that are not part of replicaset, when tryong to cprdon it, it wont work throws error. when forcefulling drained it , that particular od will be lost permanently


### 2. Kubernetes Software version:
The core of Kubernetes' control plane is the API server. The API server exposes an HTTP API that lets end users, different parts of your cluster, and external components communicate with one another.

The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes (for example: Pods, Namespaces, ConfigMaps, and Events).

Most operations can be performed through the kubectl command-line interface or other command-line tools, such as kubeadm, which in turn use the API. However, you can also access the API directly using REST calls.

- kubernetes consistes of 3 parts
```
V1.11.3
Major.minor.patch
     -Features   - Bug Fixes
     -Fubctionalities 
```
- Standard software releae
- first release v1.0
- latest stable version v1.13.0
- albha & beta release v1.10.0-alpha /v1.10.0.beta / v1.10.0


### 3. Cluster upgrades process:

```
Levl 1:     kube-apiserver
                X   v1.10

Level 2:    controller-manager           kube-schduler               kubectl
            X-1  v1.9 or v1.10           X-1  v1.9 or v1.10        X+1  > X-1

level 3:         kubelet                                 kube-proxy
          X-2  v1.8 or v1.9 or v1.10           X-2  v1.8 or v1.9 or v1.10      
```

so, kube-apiserver is the highest level and all lewer level component version cannot be hight than kube-apiserver.

- kubenetes always support latest 3 minor versions are available
- always good parctice to release before relase of 3rd version is good to upgrade cluster 

- recommanded practice to upgrade is 1 level higher than revious
- upgrades involves 2 major steps:
    1. upgrade master node
    2. upgrade worker node

    1.Upgrade master:
        - Using kubeadm:
            ```
            kubectl upgrade plan
            kubectl upgrade apply
            ```
        - when master is upgrading ,master node goes down, no new scheduling of pods/app happens. and all the pods that are in worker nodes will conttnue running.
        - when pod fails, no new pod is scheduled
        - when cluster is upgraded, functions comes normal.
    
    2. upgrade worker node:
        - strategy-1 :
            - upgrade all worker nodes at a time
            - requires down time
        - strategy-2 :
            - upgrade each node one by node
            - while upgrading each node, all workloads will be shifted to other nodes.
        - Strategy-3:
            - to add new node to the cluster
            - move all workloads to the new node.

#### KUBEADM - UPGRADE:
- Mater
    ```
    kubeadm upgrade plan
    ```
    O/P:
        - current version
        - all other info.....
    ```
    apt-get upgrade -y kubeadm=1.12.0-00
    kubeadm upgrade apply v1.12.0
    kubectl get nodes
    ```

    - Upgrade kubelet:
    ```
    apt-get upgrade -y kubelet=1.12.0-00
    sudo systemctl daemon-reload
    sudo systemctl restart kubelet
    ```
- Worker nodes:
    - First drain the node which you are upgarding on master:
    ```
    kubectl drain <node_name> --ignore-daemonsets
    ```
    - go to worker node and upgarde packages:
    ```
    apt-get update && apt-get install -y kubeadm=1.19.x-00 && \
    apt-mark hold kubeadm
    apt-get update && apt-get install -y kubelet=1.22.3-00 kubectl=1.22.3-00 && \
    kubeadm upgrade node config --kubelet-version v1.19.x-00
    sudo systemctl daemon-reload
    systemctl restart kubelet
    ```
    - uncordon the node from master
    ```
    kubectl uncordon <node_name>
    ```
    - Repeat same for other nodes


### 4. Backup and Restart methods:
- BackUp Candidate:
    1. Resource Configurations
    2. ETCD Cluster 
    3. Persistent Volumes

1. Resource Configuration:
    - Declarative approach-preffered: 
        - use all the definition files for all applications
        - use always source code repo
        - so, if we loose just run this files.
    
    - Imperative: what someone used commands to create pods.

    - kube-apiserver(quieries kube api server):
        - Get all the definitions of the app running
        ```
        kubectl get all --all-namespaces -o yaml > all-deploy-service.yaml
        ```
        - Use Tools called : `VELERO`

2. ETCD Cluster:
    - stores the info about state of the application
    - means, every info of nodes, cluster etc stores in this cluster
    - Hosted on master nodes
    - directory: `/var/lib/etcd`
    - Taking snapshot of etcd database:
        ```
        ETCDCTL_API=3 etcdctl snapshot save snapshot.db 
        ls
        ETCDCTL_API=3 etcdctl snapshot status snapshot.db 
        ```
    
    #### ETCD Restore :
    - stop kubeapi-server:
    ```
    service kube-apiserver stop
    ```

    - Restore 
    ```
    ETCDCTL_API=3 etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup
    ```
    - after running ,  new data directory is created to redirect all the service to this db
    ```
    Next, update the /etc/kubernetes/manifests/etcd.yaml: new directory: /var/lib/etcd-from-backup
    ```
    - Restart etcdb
    ```
    systemctl doemon-reload
    service etcd restart
    ```

    - start kubeapi server:
    ```
    service kube-apiserver start
    ```

### Working with ETCDCTL

- `etcdctl` is a command line client for etcd.

- make sure that you set the `ETCDCTL_API` to <verion>.

- You can do this by exporting the variable `ETCDCTL_API` prior to using the etcdctl client. This can be done as follows:
```
export ETCDCTL_API=3
```

On the Master Node:

- For example, if you want to take a snapshot of etcd, use:
```
etcdctl snapshot save -h 
```
- Since our ETCD database is TLS-Enabled, the following options are mandatory:
```
--cacert      : verify certificates of TLS-enabled secure servers using this CA bundle

--cert        : identify secure client using this TLS certificate file

--endpoints=[127.0.0.1:2379]    :This is the default as ETCD is running on master node and exposed on localhost 2379.

--key              : identify secure client using this TLS key file
```

- Similarly use the help option for snapshot restore to see all available options for restoring the backup.
```
etcdctl snapshot restore -h
```

Ex:
```
ETCDCTL_API=3 etcdctl snapshot save /opt/snapshot-boot.db \
--endpoints=127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key
```