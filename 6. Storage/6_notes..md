## Storage
---
- Persistent volume
- PVC
- configuring
- Access modes for volume
- kubernetes storage volume

### Storage In Docker:
1.  storage drivers
2.  volumes drivers

#### 1. Storage drivers:
- How Docker stores data on local file system ?
```
/var/lib/docker   # this is the path where docker stores all data
    |images
    |containers
    |volumes
    |aufs
```
- Layered architecture
    - when docker builds images, it creates a layers of each step
    - when doing another build which some steps common in previous build, docker reuses the layers .
    - image layers: read-only eg. app.py
    - when docker ran the container :container layer eg. temp.txt ,  it creates a writable layer(logs the app, copies etc) , life is till the ocntainer is alive
    - so same image layers
    - we can still modify the image layer, but docker create a copy of image in ccontainer layer as writable object: COPY-ON-WRITE

- VOLUMES:
    - docker volume create data_volume
        ```
        /var/lib/docker   # this is the path where docker stores all data
            |volumes
                |data_volume
        ```
    - `docker run -v data_volume:/var/lib/mysql mysql`    # create a new cotainers and mount the data in this volumes. If continer is not alive still the data is available in volume. : READ WRITE 
    - Use `--month` as a preffered way 
    ```
    docker run -mount type=bind source=/data/mysql, target=/var/lib/mysql mysql
    ```
    - Storage driver:AUFS,Overlay,ZFS etc

- VOLUME DRIVER Pluging in docker:
    - When using aws ebs :
        ```
        docker run -it --name mysql --volume-driver rexray/ebs --mount source=/data/mysql, target=/var/lib/mysql mysql
        ```

### 1. VOLUMES:
- Data generated by pods will be stored in Volume
- Eg.
```
spec:
    containers:
    - image:
        ----
      volumeMounts:                     # data generated from the containers will be stored in this path
        - mountPath: /opt
          name: data-volume
    volumes:
       - name: data-volume
         hostPath:
            path: /data                 # any data geneated by this pod will be fall under this directory
            type: Directory
```
`Note:` This will be applicable only for single node cluster not for multi node cluster. because node should have same directory .

- Volume Types: aws ebs ,NFS,ClusterFS etc
- eg. aws ebs
```
    volumes:
       - name: ebs-volume
         awsElasticBlockStore:
            volumeID:<VolumeID>
            fsType: ext4 
```

### 2. Persistent Volumes:
- when we created volume in above, we created in the pod. so, when we have lot of pods, we need to configure on all pods. so best way is to created pool of storage created by admistrators and make use by the users .
- eg.
```
kind: PersistentVolume
metadata:
 name: pv-vol1
spec:
    accessModes:
        - ReadWriteOnce | ReadOnlyMany  | WriteOnlyMany
    capacity:
        storage: 1Gi
    hostPath:
        path: /tmp/data                          #  not to be used in production env 
    or 
    awsElasticBlockStore:
        volumeID:<VolumeID>
        fsType: ext4   
```

### 3. Persistent Volume Claim
- admintrators creates persistent volumes & users creates persistent volume claims., they are 2 different objects in k8s namespace
- Binding:
    - Every PVC sin bind to a single persistent volumes, during this kube will find the suitable match based on the resource request. we can also use labels to bound to right PV.
    - one to one relation b/t PV & PVC
    - if no volume is available PV will be in pending state
    - eg.
    ```
    kind: PersistentVolumeClaim
    metadata:
    name: myClaim
    spec:
        accessModes:
            - ReadWriteOnce | ReadOnlyMany  | WriteOnlyMany
        resources:
            requests:
                storage: 500Mi 
    ```
    - To get the created PVCs
    ```
    kubectl get pvc 
    ```
    - when claim is created, kube will check for PV and if request matches then it will be binded

### 4. Using PVCs in PODs

- Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:
- Eg.
```
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```
- The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.

### 5. Storage Classes:
- in previous point, we have provison the volume in the aws manually .so to automatically provioning the volumes.
- when storage class is mentioned, a dynamic provisioing of volume takes place.
sc-definition.yaml
```
apiVersion:
kind: StorageClass
metadata:
    name: google-storage
provisioner: kubernetes.io/gce-pd
parameters:
    type: pd-stanard
    replication-type: none
```

pvc-definition.yaml
```
--
kind: PersistentVolumeClaim
spec:
    accessModes:
        - ReadWriteOnce
    storageClass: google-storage
    resource:
        request:
            storage: 500Mi
```

- Visit volume plugin for more info.

- storageClass types:
    - silver  -  type: pd-standard
    - gold -  type: pd-ssd
    - platinum - type: pd-ssd, replication-type: regional-pd

- Kubernetes supports two volumeModes of PersistentVolumes: `Filesystem` and `Block`.

- volumeMode is an optional API parameter. `Filesystem` is the default mode used when volumeMode parameter is omitted.


- Refer: !https://kubernetes.io/docs/concepts/storage/persistent-volumes/

`Volume Binding Mode`:
- The volumeBindingMode field controls when volume binding and dynamic provisioning should occur. When unset, "Immediate" mode is used by default.

- The Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling requirements. This may result in unschedulable Pods.

- A cluster administrator can address this issue by specifying the WaitForFirstConsumer mode which will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. PersistentVolumes will be selected or provisioned conforming to the topology that is specified by the Pod's scheduling constraints. These include, but are not limited to, resource requirements, node selectors, pod affinity and anti-affinity, and taints and tolerations.